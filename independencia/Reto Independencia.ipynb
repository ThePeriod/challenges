{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETO INDEPDENDENCIA\n",
    "\n",
    "Desarrollado por Dante Chavez como requisito para avanzar en la convocatoria para la posición de Data Scientist en Independencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACME ©\n",
    "![p1.jpg](p1.jpg)\n",
    "## Proceso de Calificación de Solicitudes de Crédito\n",
    "#### Creado para el Banco ACME que forma parte de la Corporación ACME (_**A C**ompany that **M**akes __E__verything_)\n",
    " \n",
    " \n",
    "Hola, gracias por confiar en nosotros. **El Banco para seres Humanos con corazón digital.**\n",
    "\n",
    "Tenemos el compromiso de acercar la banca a *todos*.\n",
    "Y con esta consigna te presentamos cómo evaluamos tu solicitud de préstamo de manera muy sencilla.\n",
    "\n",
    "Como debes recordar, recogimos tus datos personales para que puedan ser evaluados y otorgarte el crédito.\n",
    "De la misma forma hicimos con nuestros clientes antiguos desde que iniciamos nuestras operaciones en el país, y nuestros expertos en banca determinaron con su experiencia si estas solicitudes se __aprueban__ o si se __rechazan__.\n",
    "\n",
    "\n",
    "### Premisa:\n",
    "Las solicitudes de crédito aprobadas tanto como las desaprobadas tienen ciertas características que los diferencian, conocer cuáles son estas diferencias nos permitirá identificar si tú tienes la _capacidad de pago_ del crédito que has solicitado al reconocer si tu información personal se asemeja al grupo de aprobados o al de desaprobados.\n",
    "\n",
    "### Pero eso ya lo están haciendo con sus expertos, ¿no es así?\n",
    "Desde 1949 trabajamos con expertos para realizar la clasificacion de las solicitudes de crédito. Con el pasar del tiempo la cantidad de solicitudes se ha incrementado mucho y no tendremos suficiente personal para realizar las evaluaciones además de que empezaríamos a demorar más de lo usual.\n",
    "\n",
    "Sin embargo, en estos tiempos en que los autos no tripulados ya no son ciencia ficción, donde los celulares reconocen rostros y objetos e incluso máquinas superan el desempeño humano en juegos de mesa de alta complejidad, es posible **mejorar** nuestra forma de trabajo.\n",
    "\n",
    "En ACME estamos usando la misma tecnología que permite que todo lo mencionado sea posible, el _Machine Learning_, con el cual podremos atender **más** solicitudes **más** rápido y **mejor**.\n",
    "\n",
    "### ¿Y qué tan fácil es para una máquina evaluar mi solicitud de crédito?\n",
    "No es facil.\n",
    "\n",
    "![chihuahua-muffin-2.jpg](chihuahua-muffin-2.jpg)\n",
    "\n",
    "En esta foto, identificar un perro chihuahua de un muffin es fácil para los humanos como nosotros porque sabemos qué es un chihuahua y qué es un muffin sin importar que tan parecidos sean los chihuahuas de la foto entre sí.\n",
    "Imagina que no supiesemos qué es un perro chihuahua ni lo que es un muffin, ni siquiera lo que es un perro y solo nos mostrasen las fotos y nos indiquen que hay 2 grupos, uno es de \"perros chihuahua\" y otro es de \"muffins\".\n",
    "\n",
    "Ese es el problema que necesitamos resolver.\n",
    "\n",
    "De los datos de nuestras solicitudes de crédito pasados tenemos 2 grupos: los aprobados y los desaprobados, que, al igual que en la foto, no son exactamente iguales dentro de cada grupo, esto es, que no todos los aprobados vienen del mismo distrito o que no todos alquilan un domicilio.\n",
    "\n",
    "Cuando ingresas tu registro de solicitud queremos saber si la información que tenemos de ti te clasificaría en el grupo de aprobados o desaprobados.\n",
    "\n",
    "Ese tipo de clasificación se realizan mediante técnicas de _Machine Learning_.\n",
    "Y aquí, en ACME lo resolvemos en 4 pasos:\n",
    "\n",
    "![metod.jpg](metod.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A, de Analizar:\n",
    "\n",
    "Para poder realizar la clasificación de la manera más acertada posible debemos asegurarnos que la información que ingresa es clara y define por completo la __aprobación__ o el __rechazo__.\n",
    "\n",
    "En el paso **Analizar**, se realiza lo que a nivel técnico se conoce como **Análisis Exploratorio de Datos**. Esto consiste en analizar de manera explorativa los datos históricos que tenemos de nuestra base de datos de solicitudes, qué valores existen y que problemas podrían presentarse que nos haga pensar en el siguiente paso que es la **Limpieza de Datos**.\n",
    "\n",
    "### El Diccionario de Datos.\n",
    "Contamos con los datos de las 14 campos de nuestra base de datos. Los datos acerca de los campos se llaman _metadatos_ y son:\n",
    "\n",
    "![schema.png](schema.png)\n",
    "\n",
    "El diccionario de datos nos servirá como linea de base para evaluar los valores reales que tenemos en nuestra base de datos.\n",
    "\n",
    "### Ya entendí, entonces, ¿Cómo es la magia?\n",
    "Bien, toda la parte **A, de Analizar** es realizada en el lenguaje de programación Python, que básicamente son instrucciones escritas por un programador que la máquina interpreta y ejecuta. Independientemente del lenguaje de programación usado, el conjunto de instrucciones que tienen uno o varios propósitos determinados se conocen como *bloque de código* o simplemente *código*.\n",
    "\n",
    "El código para realizar el paso uno se puede leer en las siguientes párrafos. Se puede leer el propósito de cada línea de código a partir del símbolo \"#\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 1: A, de Analizar\n",
    "# PARTE 1: CARGA DE DICCIONARIOS Y DE DATOS.\n",
    "# Importar librerias que contienen funciones que nos hacen la Limpieza de Datos más facil.\n",
    "import numpy as np # Importa numpy y le da el alias np\n",
    "import pandas as pd # Importa pandas y le da el alias pd\n",
    "import matplotlib.pyplot as plt # Importa matplotlib y le da el alias plt\n",
    "import seaborn as sns # Importa seaborn y le da el alias sns\n",
    "from scipy.special import boxcox1p # Importa el transformador Box Cox de SciPy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_colwidth',500)\n",
    "\n",
    "# Importar Input\n",
    "file='train.csv' # Le da al archivo imput.csv un alias 'file'\n",
    "data=pd.read_csv(file, na_values='NA') # Usa la funcion read_csv de pandas para leer input.csv y además registra los valores nulos como NA\n",
    "\n",
    "# Describe información sobre todas las variables\n",
    "data.info() # Genera la información de los datos. Esto se conoce como Metadata.\n",
    "#print(df.describe(include='all')) # Muestra los datos de las 14 variables que venían en input.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del primer resultado, que llamaremos *output* a partir de ahora, podemos ver que la primera variable que es la identificacion del cliente (id) tiene 1894 valores.\n",
    "\n",
    "### ¿Por qué las demas variables no tienen los mismos 1894 valores que corresponden al total de solicitudes de crédito?\n",
    "\n",
    "Algunos solicitantes no ingresan la totalidad de los datos que se les solicita en ACME.\n",
    "\n",
    "Por ejemplo, la segunda variable, **Loan Amount** solo tiene 1800 valores no nulos, esto significa que hay 94 solicitudes de crédito en las que se omitió incluir la cantidad de dinero solicitada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 1: A, de Analizar\n",
    "# PARTE 2: ANÁLISIS EXPLORATORIO DE DATOS\n",
    "# Analisis del primer campo str: Term\n",
    "data['Term'].unique() # Entrega los valores únicos existentes en toda la base de datos para el campo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este output descubrimos que existen 2 opciones para elegir en los meses que se puede pagar la deuda:\n",
    "* 36 meses\n",
    "* 60 meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Term'].value_counts() # Entrega un conteo de cuantas veces aparece cada valor que toma el campo en la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este output vemos que la cantidad de solicitudes de 36 meses es considerablemente mayor a las de 60 meses. Apoximadamente en una razón de 10 a 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['State'].unique() # Entrega los valores únicos existentes en toda la base de datos para el campo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este output descubrimos que hay 49 estados diferentes desde donde nuestros clientes potenciales realizan su solicitud de crédito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['State'].value_counts() # Entrega un conteo de cuantas veces aparece cada valor que toma el campo en la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de este Output es muy importante, tenemos algunos estados que tienen muy pocas solicitudes por lo que sera necesario determinar qué Estados terminarían quedando dentro del modelamiento. Mi criterio para esta ocasión es seleccionar aquellos Estados que tengan 10 o más solicitudes de crédito.\n",
    "\n",
    "### ¿Por qué se tendría que hacer eso?\n",
    "Recuerda que nuestra solucion al problema requiere que, sin tener definidas las características de quiénes son los clientes pagadores ni los deudores podamos descubrir estas a partir de nuestra base de datos histórica. Entonces, tener valores demasiado específicos no nos ayuda a **generalizar**.\n",
    "\n",
    "La solución, que es el modelo resultante, necesariamente tiene que generalizar nuevos casos, y al contrario de lo que se puede pensar, un modelo muy específico termina siendo menos útil que uno que puede generalizar.\n",
    "\n",
    "### ¿Puedes explicarlo con un ejemplo?\n",
    "Claro, en nuestra base de datos tenemos entre los 1894 solicitudes de credito la que tiene id 824, es la unica solicitud de un cliente que viene de Dakota del Norte, alquila una casa, tiene un ingreso mensual de 42000, cantidad que no ha sido verificada y se le ha otorgado el crédito.\n",
    "\n",
    "Si tu tuvieras que otorgar el prestamo y teniendo este solo caso único podrías decir que ¿Todas las personas que compartan estas precisas características deberían tener aprobadas sus solicitudes?.\n",
    "\n",
    "De la misma manera en que no es verdad que _todos los hombres son iguales_ y en general, cualquier afirmación _non plus ultra_ (Todos los X son Y o Ningún A es B) suele ser inválida. No es posible determinar si la siguiente persona con las mismas caracteristicas debería ser acreedora a un crédito.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data.State=='ND'] - Este es el ejemplo en el parrafo anterior.\n",
    "data['Income Verification Status'].value_counts() # Entrega un conteo de cuantas veces aparece cada valor que toma el campo en la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectvamente, los valores que puede tomar el Estado de Verificación del Ingreso son:\n",
    "* Parcialmente Verificado\n",
    "* Verificado\n",
    "* No Verificado\n",
    "\n",
    "No hay muchas diferencias entre las frecuencias de cada una de las opciones por lo que podemos decir que _tienen un buen balance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Home Ownership'].value_counts() # Entrega un conteo de cuantas veces aparece cada valor que toma el campo en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Home Ownership', hue='Approve Loan', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Output también es importante. De manera intencional juntaré los valores de ANY con los de OWN ya que los puntos de datos de esta son muy bajos. Este cambio se realiza en el paso 2: C, de Corregir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Loan Purpose'].value_counts() # Entrega un conteo de cuantas veces aparece cada valor que toma el campo en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Due Settlement'].value_counts() # Entrega un conteo de cuantas veces aparece cada valor que toma el campo en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Payment Plan'].value_counts() # Entrega un conteo de cuantas veces aparece cada valor que toma el campo en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asimetría de No Aprobados vs. Aprobados.\n",
    "data['Approve Loan'].value_counts()[0]/len(data['Approve Loan']) # Cantidad de No Aprobados vs. Aprobados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este resultado es muy bueno, no hay problemas de desbalance con la variable sobre la que queremos hacer predicción. Casi llega al 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media de préstamos solicitado por grupo de Aprobados y Rechazados.\n",
    "data[['Loan Amount','Approve Loan']].groupby(by='Approve Loan').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digrama de Caja de Prestamos Solicitados entre Aprobados y Rechazados.\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data['Loan Amount'],data['Approve Loan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C, de Corregir:\n",
    "\n",
    "Una vez terminada la exploración y el análisis de los datos con los que contamos en nuestra base de datos podemos decir que **conocemos** los datos, esto quiere decir que hemos identificado que variables pueden causarnos problemas y que consideraciones debemos tomar en cuenta antes de pasar nuestros datos por el algoritmo que se encargará de clasificar una nueva solicitud de crédito.\n",
    "\n",
    "Retomemos el ejemplo de los chihuahuas, si tuviesemos un chihuahua que sale junto a un auto, donde el auto ocupa mas de la mitad del espacio de la foto, esto no nos ayudaría a la clasificación, sino que, al contrario, la complicaría aún mas. Tendriamos que editar la foto para que la imagen del chihuahua aporte siendo una imagen válida.\n",
    "\n",
    "Las operaciones que se realizan en este paso son:\n",
    "* **Imputación de Datos** : Se trata de completar los datos nulos con algún valor representativo a fin de no perder el aporte de esa solicitud de créditos a las demás variables.\n",
    "* **Corrección de los Errores Identificados**: A partir de las notas que se tomaron en la parte A se procede corrigiendo ya sea tranformando las variables o eliminandolas.\n",
    "* **Generación de Variables Dummy**: A partir de las variables categóricas se procede a crear nuevas variables. Se realiza en dos tiempos, primero se generan las variables dummy o variables alternativas y luego se retiran las variables originales de la base de datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: C, de Corregir\n",
    "# PARTE 1: ELIMNACION DE VARIABLES\n",
    "# Ver los nulos de cada variable/campo\n",
    "d={'Cabecera':[],'Nulos':[]} # Inicializador de Nombres de Columnas y de Cantidad de Nulos en un array\n",
    "# Bloque recursivo para identficar tanto los nombres de las columnas como la cantidad de nulos existentes\n",
    "for i in data.columns:\n",
    "    d['Cabecera'].append(i)\n",
    "    d['Nulos'].append(len(data[data[i].isnull()][i]))\n",
    "nulos=pd.DataFrame(d) # Conversion de array a dataframe\n",
    "nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Output de este bloque nos indica que la cantidad de Nulos es muy baja en todos los casos. Un criterio para la Elminación de Variables es determinar si la cantidad de nulos existentes supera el 50% de todos los datos. Entonces, según este criterio no eliminamos ninguna variable por exceso de nulos. Sino que se realizará **Imputación de Datos** que consiste en otorgarle un valor de las opciones existentes (en el caso de variables categoricas) o reemplaza con el promedio (en el caso de numéricas) al que en un principio era nulo. Las siguientes variables serán susceptibles de Imputación:\n",
    "* Loan Amount\n",
    "* Term\n",
    "* Annual Income\n",
    "* Income Verification Status\n",
    "* Average Account Balance\n",
    "* Due Amount\n",
    "* Home Ownership\n",
    "* Due Settlement\n",
    "* Installment Amount\n",
    "* Payment Plan\n",
    "\n",
    "Otro criterio de eliminación de variables es el retirar las constantes. Como vimos en un bloque anterior, en ninguna solicitud de crédito se ha pedido refinanciamiento, por lo que, la variable **Payment Plan** se retira del análisis. También **Id** por no hacer diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminacion de variable\n",
    "data.drop(['id','Payment Plan'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: C, de Corregir\n",
    "# PARTE 2: IMPUTACION DE DATOS\n",
    "cols_con_nulos_int=['Loan Amount', 'Annual Income', 'Average Account Balance', 'Due Amount', 'Installment Amount']\n",
    "# Revision de Histogramas para poder ver con qué se puede imputar cada variable.\n",
    "for col in cols_con_nulos_int:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.distplot(data[data[col].notnull()][col])\n",
    "    plt.title('Histograma de {}'.format(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La asimetría presente nos indica que es más conveniente realizar la Imputación mediante reemplazo con la mediana.\n",
    "\n",
    "Creamos una función en Python que realice el trabajo de imputación para aplicarlo a todas las variables que tienen por lo menos un valor nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función input_mediana permite hacer el reemplazo de los valores nulos dentro cada columna por el valor de la mediana de esa columna.\n",
    "def imput_mediana(data,col):\n",
    "    med=data[col].median()\n",
    "    data[col]=data[col].apply(lambda x: med if np.isnan(x) else x)\n",
    "    \n",
    "for col in cols_con_nulos_int:\n",
    "    imput_mediana(data,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de ejecutar la función de imputación a todas las columnas tenemos solo las variables categóricas con valores nulos pendientes.\n",
    "\n",
    "### ¿Qué se hace con las variables que no nos numéricas pero que si tienen valores nulos?\n",
    "Primero, hay que revisar que se hayan corregido lo que se encontró en el paso 1: A, de Análisis.\n",
    "Posteriormente lo que se hace es crear Variables Alternativas conocidas también como _Variables Dummy_ que estan basadas en los valores originales de las variables no numéricas.\n",
    "La función get_dummies de la biblioteca _pandas_ realiza esa operación por nosotros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: C, de Corregir\n",
    "# PARTE 3: Tratamiento de los problemas encontrados en el paso 1: A, de Analizar.\n",
    "# 'Term': Diferencia en sus frecuencias dentro de la base de datos de 10 a 37, a pesar del desbalance,\n",
    "# por ser solo 2 valores pasa directo al generador de variables \n",
    "# 'State': Determinar los valores que si quedaran dentro del análisis y generarle sus dummies + eliminar originales.\n",
    "estados_dentro=['CA','TX','NY','FL','NJ','NC','MI','PA','MD','AZ','MA','IL','VA','MN','AL','GA','TN','CO','OH','WA','IN','WI','NV'\n",
    "               ,'MO','LA','OK','UT','AR','MS','OR','NE','KY','SC','CT','HI','NM','KS']\n",
    "for i in estados_dentro:\n",
    "    data['State'+i]=data['State'].apply(lambda x:1 if x==i else 0)\n",
    "data.drop('State',axis=1,inplace=True)\n",
    "# 'Income Verification Status': Buen balance, van a pasar por el generador de variables sin mayor cambio.\n",
    "# 'Home Ownership': Corregir el desbalance con ANY pasandolo a OWN.\n",
    "data['Home Ownership']=data['Home Ownership'].apply(lambda x: 'OWN' if x in ['ANY'] else x)\n",
    "# 'Due Settlement': Buen balance, van a pasar por el generador de variables sin mayor cambio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "habiendo tratado a **States** solo restan:\n",
    "* Term\n",
    "* Income Verification Status\n",
    "* Home Ownership\n",
    "* Due Settlement\n",
    "\n",
    "Recordemos que son 2 movimientos dentro de la creacion de dummies:\n",
    "* Generar los Dummies.\n",
    "* Eliminar los originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: C, de Corregir\n",
    "# PARTE 4: Creacion de Variables Alternativas (Dummy)\n",
    "cols_con_nulos_cat=['Term', 'Income Verification Status', 'Home Ownership', 'Due Settlement','Loan Purpose'] # Purpose no tiene nulos pero es cat\n",
    "for col in cols_con_nulos_cat:\n",
    "    data=data.join(pd.get_dummies(data[col],drop_first=True))\n",
    "data.drop(cols_con_nulos_cat,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M, de Modelar:\n",
    "\n",
    "Ya estamos en la parte de modelamiento.\n",
    "Una vez que sabemos qué le hacemos a los datos, replicamos esto a los datos de testeo.\n",
    "Y luego mediante un algoritmo de Aprendizaje Automático (Machine Learning) procedemos con la predicción de los valores en **Approved Loan** en base a lo que conocemos sobre la base de datos histórica.\n",
    "\n",
    "Las actividades que se realizan en este paso son:\n",
    "* **Integrar los pasos de Analizar y Corregir**: Ya que se han hecho varios cambios a los datos de entrenamiento, replicamos esos cambios a los datos de testeo.\n",
    "* **Pase de los Datos a los Algoritmos de Modelamiento**: Ya que tenemos preparados los datos de entrenamiento y de testeo procesados, procedemos con ingestar a los algoritmos de _Machine Learning_ con ellos a fin de obtener un modelo predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defincion de X y Y en train.csv para ajustar a los modelos.\n",
    "data_train=data\n",
    "y_train=data_train['Approve Loan']\n",
    "data_train.drop('Approve Loan',axis=1, inplace=True)\n",
    "X_train=data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replica del tratamiento de datos a los datos de testeo (test.csv)\n",
    "data_test=pd.read_csv('test.csv')\n",
    "output_id=data_test['id']\n",
    "data_test.drop(['id','Payment Plan'], axis=1, inplace=True) # Eliminacion de variable en test\n",
    "for col in cols_con_nulos_int:\n",
    "    imput_mediana(data_test,col) # Imputacion de datos con la mediana en test\n",
    "for i in estados_dentro:\n",
    "    data_test['State'+i]=data_test['State'].apply(lambda x:1 if x==i else 0) # Creación recursiva de variables dummy con base en State para test\n",
    "data_test.drop('State',axis=1,inplace=True) # Eliminacion de State en test\n",
    "data_test['Home Ownership']=data_test['Home Ownership'].apply(lambda x: 'OWN' if x in ['ANY'] else x) # 'Home Ownership': Corregir el desbalance con ANY pasandolo a OWN.\n",
    "for col in cols_con_nulos_cat: # Creación recursiva de varables dummy para el resto de varibles categóricas.\n",
    "    data_test=data_test.join(pd.get_dummies(data_test[col],drop_first=True))\n",
    "data_test.drop(cols_con_nulos_cat,axis=1,inplace=True) # Eliminacion de variable categóricas originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de X e Y en test.csv para ajustar a los modelos\n",
    "X_test=data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos de _Machine Learning_\n",
    "El problema al que nos enfrentamos es un problema de clasificación.\n",
    "Los algoritmos de _Machine Learning_ más usados para este tipo de problemas son:\n",
    "\n",
    "* Regresión Logistica.\n",
    "* Random Forest (Bosques aleatorios)\n",
    "* Light GBM (Modelos de Aumento de Gradiente)\n",
    "\n",
    "La teoría detras de estos algoritmos descanza en las disciplinas de la estadística y las ciencias de la computación, específicamente a la Teoría de la Información y tiene, como tal, una complejidad científica moderada.\n",
    "\n",
    "El modelo final es un modelo de ensamble por votos, que no es nada más que determinar por mayoria la clase a la que pertenecerá una solicitud de crédito.\n",
    "\n",
    "Por ejemplo, si el Light GBM y el Random Forest nos dicen que una solicitud no debe ser aprobada pero la Regresión Logistica indica que si, concluimos la evaluación del modelo como **rechazo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import learning_curve,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Logistica:\n",
    "lr=LogisticRegression(C=10, tol=0.001, random_state=51, n_jobs=-1, solver='liblinear', class_weight='balanced')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rfc=RandomForestClassifier(n_estimators=150, max_depth=14, max_leaf_nodes=None, min_impurity_decrease=1e-05, min_samples_leaf=100, class_weight='balanced'\n",
    "                          ,n_jobs=-1, random_state=51)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM\n",
    "lgb=LGBMClassifier(random_state=51, class_weight={0:1, 1:4}, colsample_bytree=1, max_depth=7\n",
    "                   , min_child_samples=20, min_split_gain=0, n_estimators=100, reg_alpha=13)\n",
    "lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos los 3 modelos, ahora los pondremos a prueba con los datos de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresion Logistica\n",
    "y_test_pred_lr=lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "y_test_pred_rfc=rfc.predict(X_test)\n",
    "\n",
    "# LightGBM\n",
    "y_test_pred_lgb=lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultado final - Votación\n",
    "resultado=pd.DataFrame(index=X_test.index)\n",
    "resultado['lr']=y_test_pred_lr\n",
    "resultado['rfc']=y_test_pred_rfc\n",
    "resultado['lgb']=y_test_pred_lgb\n",
    "resultado['cuenta']=resultado.sum(axis=1)\n",
    "resultado['Approve Loan']=resultado['cuenta'].apply(lambda x:1 if x>=3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el output para revision de Independencia.\n",
    "outcome=pd.concat([output_id,resultado['Approve Loan']],axis=1)\n",
    "outcome.to_csv('outcome.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E, de Evaluar:\n",
    "\n",
    "Lo usual es que en el conjunto de datos de test tengamos unos valores reales de la variable a predecir, en este caso sobre la aprobación o rechazo de las solicitudes de crédito.\n",
    "\n",
    "Las gráficas para evaluar el desempeño del modelo respecto de la realidad son:\n",
    "\n",
    "* El Área Bajo la Curva (AUC)\n",
    "* Característica Operativa del Receptor (ROC)\n",
    "\n",
    "Sin embargo no contamos con los valores reales para el conjunto de datos de testeo por lo que no se podrá mostrar dichas curvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que existiese los valores reales de aceptación o rechazo de la solicitud, se almacenarian en y_test.\n",
    "# Este sería el código para mostrar los gráficos.\n",
    "print(classification_report(y_test, resultado['y_pred']))\n",
    "print('ROC-AUC:', round(roc_auc_score(y_test, resultado['y_pred']),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glosario\n",
    "__Campo__: Es una columna que descibe cierta característica de la solicitud de crédito.\n",
    "\n",
    "__Variable__: Campo\n",
    "\n",
    "__Variable Categórica__: Es una variable que no es numérica, por ejemplo _State_ que indica el lugar de donde proviene la solicitud de crédito."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
